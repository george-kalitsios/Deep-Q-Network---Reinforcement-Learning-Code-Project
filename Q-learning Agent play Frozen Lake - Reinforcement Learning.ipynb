{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beeda640",
   "metadata": {},
   "source": [
    "Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af29f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fdedb",
   "metadata": {},
   "source": [
    "Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "068819a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7727e3",
   "metadata": {},
   "source": [
    "Create the Q-table and initialize it \n",
    "\n",
    "Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "OpenAI Gym provides us a way to do that: env.action_space.n and env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "998ceb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a991bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_size, action_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fcf126",
   "metadata": {},
   "source": [
    "#### Algorithm parameters\n",
    "Here, we'll specify the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7aff6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000           # Total episodes\n",
    "max_steps_per_episode = 100    # Max steps per episode\n",
    "\n",
    "learning_rate = 0.1           # Learning rate\n",
    "discount_rate = 0.99          # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "exploration_rate = 1                   # Exploration rate\n",
    "max_exploration_rate = 1               # Exploration probability at start\n",
    "min_exploration_rate = 0.01            # Minimum exploration probability \n",
    "exploration_decay_rate = 0.001         # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8eddd",
   "metadata": {},
   "source": [
    "Coding The Q-Learning Algorithm Training Loop\n",
    "### Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        # Take new action\n",
    "        # Update Q-table\n",
    "        # Set new state\n",
    "        # Add new reward        \n",
    "\n",
    "    # Exploration rate decay   \n",
    "    # Add current episode reward to total rewards list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb792a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.058000000000000045\n",
      "2000 :  0.22800000000000017\n",
      "3000 :  0.4140000000000003\n",
      "4000 :  0.5700000000000004\n",
      "5000 :  0.6470000000000005\n",
      "6000 :  0.6560000000000005\n",
      "7000 :  0.6850000000000005\n",
      "8000 :  0.7040000000000005\n",
      "9000 :  0.6620000000000005\n",
      "10000 :  0.6650000000000005\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards_all_episodes = []\n",
    "\n",
    "#  first for-loop contains everything that happens within a single episode. \n",
    "#  This second nested loop contains everything that happens for a single time-step.\n",
    "\n",
    "# Q- Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    # initialize new episode params\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            #exploitation (taking the biggest Q value for this state)\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            #Else doing a random choice --> exploration\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # The function step() returns a tuple containing the new state, the reward for the action we took, \n",
    "        #         whether or not the action ended our episode, and diagnostic information regarding our environment, \n",
    "        #         which may be helpful for us if we end up needing to do any debugging.\n",
    "        \n",
    "        # Update Q-table for Q(s,a) : [(1-a)*old_value+a*learned_value]\n",
    "        q_table[state, action] = q_table[state, action]*(1-learning_rate ) + learning_rate * ( reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode +=reward\n",
    "                                                                                              \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break        \n",
    "                                                                                              \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)  \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    " # Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000                                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fdc25",
   "metadata": {},
   "source": [
    "From this printout, we can see our average reward per thousand episodes did indeed progress over time. \n",
    "When the algorithm first started training, the first thousand episodes only averaged a reward of 0.05, \n",
    "but by the time it got to its last thousand episodes, the reward drastically improved to 0.65. \n",
    "\n",
    "Our agent played 10,000 episodes. At each time step within an episode, the agent received a reward of 1 if it reached the frisbee, otherwise, it received a reward of 0. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
    "\n",
    "So, that means for each episode, the total reward received by the agent for the entire episode is either 1 or 0. So, for the first thousand episodes, we can interpret this score as meaning that 5%\n",
    "of the time, the agent received a reward of 1 and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning\n",
    "65%\n",
    "of the time.\n",
    "\n",
    "By analyzing the grid of the game, we can see it's a lot more likely that the agent would fall in a hole or perhaps reach the max time steps than it is to reach the frisbee, so reaching the frisbee 65%\n",
    "of the time isn't too shabby, especially since the agent had no explicit instructions to reach the frisbee. It learned that this is the correct thing to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c5c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.56651888 0.47726023 0.47557092 0.47259415]\n",
      " [0.32582891 0.30168938 0.34903522 0.48050215]\n",
      " [0.40513376 0.40875344 0.40643236 0.43960892]\n",
      " [0.34950421 0.2179258  0.22658587 0.42347658]\n",
      " [0.57995993 0.41259932 0.38440829 0.43633523]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18186762 0.16966767 0.33757415 0.12303268]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37902219 0.40392625 0.3995311  0.62214574]\n",
      " [0.38632498 0.67204516 0.31229379 0.40910617]\n",
      " [0.61871364 0.43214808 0.34375371 0.32742847]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.57159051 0.49424274 0.75323657 0.38604726]\n",
      " [0.68890127 0.86400588 0.69426378 0.70536482]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ec747",
   "metadata": {},
   "source": [
    "#### Watch our agent play Frozen Lake by playing the best action \n",
    "#### from each state according to the Q-table\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                # Agent reached the goal and won episode\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode            \n",
    "\n",
    "        # Set new state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef34dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "    state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06161526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
